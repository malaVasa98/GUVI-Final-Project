{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Problem Statement**\n",
        "\n",
        "The project emphasises on creating a multifunctional tool that enables users to select and utilize different pre-trained from Hugging Face for tasks like text summarization, next word prediction, story prediction, chatbot, sentiment analysis, question answering, and image generation. A front end would be implemented which allows the user to select the task and input the required text or image for processing."
      ],
      "metadata": {
        "id": "aR2Fxaf5Uom3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np5kRbLtoo92",
        "outputId": "0a2cc1e3-7ae2-40c6-b2ce-989e5dd33efe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformers architecture to perform NLP tasks\n",
        "!pip install transformers torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-kWGYEoo85U",
        "outputId": "577bd96f-6340-4e94-93a6-0c3287072e22"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekTtk4lOXurd",
        "outputId": "b732fe26-28a7-4e5e-b1e9-9ec33c418964"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting diffusers\n",
            "  Downloading diffusers-0.30.3-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (8.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.24.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.4.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (10.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.12.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.20.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2024.8.30)\n",
            "Downloading diffusers-0.30.3-py3-none-any.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diffusers\n",
            "Successfully installed diffusers-0.30.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit"
      ],
      "metadata": {
        "id": "V35p71KTX-Z4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d380cb3e-00ee-44a4-8062-8a6d61797814"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Hugging_face.py\n",
        "# Importing necessary packages\n",
        "import torch\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from transformers import pipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import streamlit as st\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "#import matplotlib.image as mpimg\n",
        "\n",
        "st.title(':green[Multifunctional NLP and Image Generation Tool using Hugging Face Models]')\n",
        "st.write('''In this app, the user can select and utilize different pre-trained from Hugging Face\n",
        "for tasks like text summarization, next word prediction, story prediction, chatbot, sentiment analysis, question answering, and image generation.''')\n",
        "# Task selection\n",
        "task = st.sidebar.selectbox('Choose a task', ['Text Summarization', 'Next Word Prediction',\n",
        "                                              'Story Prediction', 'Chatbot',\n",
        "                                              'Sentiment Analysis', 'Question Answering',\n",
        "                                              'Image Generation'])\n",
        "\n",
        "if task=='Text Summarization':\n",
        "  user_input = st.text_area(f'Enter input for {task}')\n",
        "  if st.button('Submit'):\n",
        "    with st.spinner(f\"Processing {task}...\"):\n",
        "      model_name = \"facebook/bart-large-cnn\"\n",
        "      tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "      model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "      # Set the model to evaluation mode\n",
        "      model.eval()\n",
        "\n",
        "      def summarize(text, model, tokenizer, max_length=200, min_length=30, do_sample=False):\n",
        "        inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "        summary_ids = model.generate(inputs, max_length=max_length, min_length=min_length, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        return summary\n",
        "\n",
        "      summary = summarize(user_input, model, tokenizer)\n",
        "      st.write(\"\\nSummary:\\n\", summary)\n",
        "\n",
        "elif task=='Next Word Prediction':\n",
        "  user_input = st.text_area(f'Enter input for {task}')\n",
        "  if st.button('Submit'):\n",
        "    # Load the tokenizer and model\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "    def predict_next_word(prompt, model, tokenizer, top_k=5):\n",
        "      # Tokenize input text\n",
        "      inputs = tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "      # Generate predictions\n",
        "      with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "      # Get logits of the last token in the sequence\n",
        "      next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "      # Filter the top k tokens by their probability\n",
        "      top_k_tokens = torch.topk(next_token_logits, top_k).indices[0].tolist()\n",
        "\n",
        "      # Decode the top k tokens to their corresponding words\n",
        "      predicted_tokens = [tokenizer.decode([token]) for token in top_k_tokens]\n",
        "\n",
        "      return predicted_tokens\n",
        "    predicted_words = predict_next_word(user_input, model, tokenizer)\n",
        "    st.write(f\"Next word predictions: {predicted_words}\")\n",
        "\n",
        "elif task=='Story Prediction':\n",
        "  user_input = st.text_area(f'Enter input for {task}')\n",
        "  if st.button('Submit'):\n",
        "    with st.spinner(f\"Processing {task}...\"):\n",
        "      story_predictor = pipeline('text-generation', model='gpt2')\n",
        "      prediction = story_predictor(user_input,max_length=1000,truncation=True,clean_up_tokenization_spaces=True)\n",
        "      st.write(prediction[0]['generated_text'])\n",
        "\n",
        "elif task=='Chatbot':\n",
        "  # Load pre-trained model and tokenizer with padding set to left\n",
        "  model_name = \"microsoft/DialoGPT-medium\"\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "  # Set the model to evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  def chat_with_model(prompt, chat_history_ids, model, tokenizer):\n",
        "    # Encode the new user input, add the eos_token and return a tensor in PyTorch\n",
        "    new_user_input_ids = tokenizer.encode(prompt + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "    # Append the new user input tokens to the chat history\n",
        "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if chat_history_ids is not None else new_user_input_ids\n",
        "\n",
        "    # Generate a response\n",
        "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    # Decode the response and print it\n",
        "    response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "\n",
        "    return response, chat_history_ids\n",
        "\n",
        "  # Initialize chat history\n",
        "  chat_history_ids = None\n",
        "\n",
        "  st.write(\"Start chatting with the bot (type 'quit' to stop)\")\n",
        "  keyz = 's1'\n",
        "  user_input = st.text_input(\"You: \",key = keyz)\n",
        "  while user_input.lower() != 'quit':\n",
        "    response, chat_history_ids = chat_with_model(user_input, chat_history_ids, model, tokenizer)\n",
        "    if (response != '')and(user_input != ''):\n",
        "      st.write(f\"Bot: {response}\")\n",
        "      keyz += '1'\n",
        "      user_input = st.text_input(\"You: \",key = keyz)\n",
        "    #response = ''\n",
        "\n",
        "elif task=='Sentiment Analysis':\n",
        "  user_input = st.text_area(f'Enter input for {task}')\n",
        "  if st.button('Submit'):\n",
        "    with st.spinner(f\"Processing {task}...\"):\n",
        "      # Load pre-trained sentiment-analysis pipeline\n",
        "      sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "      def analyze_sentiment(texts):\n",
        "        results = sentiment_analysis(texts)\n",
        "        for text, result in zip(texts, results):\n",
        "          st.write(f\"Text: {text}\")\n",
        "          st.write(f\"Sentiment: {result['label']}, Score: {result['score']:.4f}\")\n",
        "\n",
        "      # Example texts for sentiment analysis\n",
        "      sent_texts = re.split(r'([.!?])', user_input)\n",
        "      resultz = [sent_texts[i] + sent_texts[i+1] for i in range(0, len(sent_texts)-1, 2)]\n",
        "      # Perform sentiment analysis\n",
        "      for text in resultz:\n",
        "        analyze_sentiment(text)\n",
        "\n",
        "elif task=='Question Answering':\n",
        "  user_input = st.text_area(f'Enter input for {task}')\n",
        "  question = st.text_input('Enter a Question ')\n",
        "  if st.button('Submit'):\n",
        "    with st.spinner(f\"Processing {task}...\"):\n",
        "      # Load the pre-trained model and tokenizer\n",
        "      model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        "      tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "      model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "      def answer_question(question, context):\n",
        "        # Tokenize input question and context\n",
        "        inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
        "\n",
        "        # Get input ids and attention mask\n",
        "        input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "        # Get the model's output\n",
        "        outputs = model(**inputs)\n",
        "        answer_start_scores = outputs.start_logits\n",
        "        answer_end_scores = outputs.end_logits\n",
        "\n",
        "        # Get the most likely beginning and end of answer with the argmax of the score\n",
        "        answer_start = torch.argmax(answer_start_scores)\n",
        "        answer_end = torch.argmax(answer_end_scores) + 1\n",
        "\n",
        "        # Convert tokens to the answer\n",
        "        answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "\n",
        "        return answer\n",
        "\n",
        "      # Define a context and a question\n",
        "      context = user_input\n",
        "\n",
        "      # Get the answer\n",
        "      answer = answer_question(question, context)\n",
        "      st.write(f\"Answer: {answer}\")\n",
        "\n",
        "elif task=='Image Generation':\n",
        "  user_input = st.text_area(f'Enter input for {task}')\n",
        "  if st.button('Submit'):\n",
        "    with st.spinner(f\"Processing {task}...\"):\n",
        "      # Load the Stable Diffusion model\n",
        "      pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\").to(\"cuda\")\n",
        "\n",
        "      # Function to generate images\n",
        "      def generate_image(prompt):\n",
        "        image = pipe(prompt).images[0]\n",
        "        return image\n",
        "      # Example text prompt\n",
        "      prompt = user_input\n",
        "\n",
        "      # Generate image\n",
        "      image = generate_image(prompt)\n",
        "\n",
        "      # Display the generated image\n",
        "      st.image(image)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AnRBgB8ZU5Z7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5256961d-1136-46e6-a204-110d718723b7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Hugging_face.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "id": "GMLhGpdHY1IZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "418cbd0e-3f2a-4329-fd25-b250989bcc01"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\n",
            "up to date, audited 23 packages in 2s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "2 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qK4xCD-Qki5G",
        "outputId": "68717851-223b-4005-f02e-baf64eb85a89"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Password/Enpoint IP for localtunnel is: 34.83.139.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/Hugging_face.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "3dDOfY2qkm2h"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Yt8kevBlDSg",
        "outputId": "9cc81f24-f8c2-4f1a-8c1b-9939fb29e541"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "your url is: https://ripe-cases-hang.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v70Nv9wcxjaz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}